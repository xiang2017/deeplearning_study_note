{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 应用 numpy 实现部分函数\n",
    "\n",
    "一下使用 numpy 实现机器学习常用的一些函数，比如 sigmoid、softmax。。。\n",
    "\n",
    "详细内容可以参考[python与numpy基础](../coursera.org/week2/Python%2BBasics%2BWith%2BNumpy%2Bv3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - sigmoid function - np.exp()\n",
    "\n",
    "提示：$sigmoid(x) = \\frac{1}{1 + e^{-x}}$ \n",
    "\n",
    "sigmoid function 是机器学习中常用的非线性函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9525741268224334\n",
      "0.952574126822\n",
      "[ 0.73105858  0.88079708  0.95257413]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    使用 math 库实现的 sigmoid 函数\n",
    "    \n",
    "    参数：\n",
    "    x - 标量\n",
    "    \n",
    "    返回：\n",
    "    s - sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "    return s\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    使用 numpy 库实现 sigmoid 函数\n",
    "    \n",
    "    参数：\n",
    "    x - 标量或者 ndarray\n",
    "    \n",
    "    返回：\n",
    "    s - sigmoid(x) 标量或者 ndarray\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s\n",
    "\n",
    "print(basic_sigmoid(3))\n",
    "print(sigmoid(3))\n",
    "\n",
    "print(sigmoid(np.array([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - sigmoid 斜率（导数）\n",
    "\n",
    "sigmoid 导数公式\n",
    "\n",
    "$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19661193  0.10499359  0.04517666]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    sigmoid 导数\n",
    "    \n",
    "    参数：\n",
    "    x - 标量或者 ndarray\n",
    "    \n",
    "    返回：\n",
    "    ds - sigmoid 导数\n",
    "    \"\"\"\n",
    "    s = sigmoid(x)\n",
    "    ds = s * (1-s)\n",
    "    return ds\n",
    "\n",
    "print(sigmoid_derivative(np.array([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - reshape array\n",
    "\n",
    "np.shape 和 np.reshape() 是常用的两个属性（方法）\n",
    "* np.shape 能够得到 ndarray/matrix 的 shape(维度)\n",
    "* np.reshape 常用来把 ndarray/matrix 改成其他 shape\n",
    "\n",
    "例如：在科学计算中，图片常用三维数组来表示（高，宽，深度-rgb = 3），有时为了计算方便，常会把三维数组重塑成 (高*宽*深度, 1) 的二维数组。\n",
    "\n",
    "实现**image2vector**函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.67826139]\n",
      " [ 0.29380381]\n",
      " [ 0.90714982]\n",
      " [ 0.52835647]\n",
      " [ 0.4215251 ]\n",
      " [ 0.45017551]\n",
      " [ 0.92814219]\n",
      " [ 0.96677647]\n",
      " [ 0.85304703]\n",
      " [ 0.52351845]\n",
      " [ 0.19981397]\n",
      " [ 0.27417313]\n",
      " [ 0.60659855]\n",
      " [ 0.00533165]\n",
      " [ 0.10820313]\n",
      " [ 0.49978937]\n",
      " [ 0.34144279]\n",
      " [ 0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    把三维图像转换成 (n, 1) 维\n",
    "    \n",
    "    参数：\n",
    "    image - 三维图像数组\n",
    "    \n",
    "    返回：\n",
    "    v - reshape 后的数组\n",
    "    \"\"\"\n",
    "    v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2], 1)\n",
    "    return v\n",
    "\n",
    "print(image2vector(np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - 输入数据正则化\n",
    "\n",
    "机器学习中常用的另一个技巧是输入数据正则化。数据正则化之后，在梯度下降过程中通常会有更好的表现。\n",
    "\n",
    "行正则化，指的是每行内容除以每行的基准值 $\\frac{x}{\\|x\\|}$。\n",
    "例如：\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 3 & 4 \\\\\n",
    "2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "则，$$\\|x\\| = np.linalg.norm(x, axis=1, keepdims=True) = \\begin{bmatrix}\n",
    "5 \\\\\n",
    "\\sqrt{56} \\\\\n",
    "\\end{bmatrix}$$\n",
    "得到 $$ x\\_normalized = \\frac{x}{\\|x\\|} = \\begin{bmatrix}\n",
    "0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "\\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "实现 **normalizeRows** 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.6         0.8       ]\n",
      " [ 0.26726124  0.80178373  0.53452248]]\n"
     ]
    }
   ],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    正则化 x\n",
    "    \n",
    "    参数：\n",
    "    x - ndarray\n",
    "    \n",
    "    返回：\n",
    "    x - 正则化后的 x\n",
    "    \"\"\"\n",
    "    norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)\n",
    "    x = x / norm\n",
    "    return x\n",
    "\n",
    "print(normalizeRows(np.array([[0, 3, 4],[2, 6, 4]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - 广播和 softmax\n",
    "\n",
    "[numpy广播](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
    "\n",
    "**softmax** 是一种常用用作二分或者多分类的算法。\n",
    "\n",
    "**说明**：\n",
    "* 对于 $x \\in \\mathbb{R}^{1 \\times n}$，则 softmax 的结果为 $$ softmax(x) = softmax(\\begin{bmatrix}\n",
    "x_1 & x_2 ... &x_n\n",
    "\\end{bmatrix}\n",
    ") = \\begin{bmatrix}\n",
    "\\frac{e^{x_1}}{\\sum_{j}{e^{e_j}}} &&\n",
    "\\frac{e^{x_2}}{\\sum_{j}{e^{e_j}}} &&\n",
    "...\n",
    "\\frac{e^{x_1}}{\\sum_{j}{e^{e_j}}} &&\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "* 对于矩阵 $\\mathbb{R}^{m \\times n}$，则 $$softmax(x) = \\begin{bmatrix}\n",
    "softmax(first row) \\\\\n",
    "softmax(second row) \\\\\n",
    "...\n",
    "softmax(last row) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "**softmax** 函数实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04\n",
      "    1.21052389e-04]\n",
      " [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04\n",
      "    8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x)\n",
    "    \n",
    "    sum_exp_x = np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    s = exp_x / sum_exp_x\n",
    "    return s\n",
    "\n",
    "print(softmax(np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2）向量化\n",
    "\n",
    "在深度学习项目里，我们经常会处理非常大的数据。如果没有很好的处理方法，将会导致程序和结果遇到很大的瓶颈。为了提高计算效率，需要引入向量化计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot=2258\n",
      " --- 循环算法计算用时：0.1389999999998892ms\n",
      "dot=2258\n",
      " --- 向量化算法计算用时：0.06899999999987472ms\n",
      "\n",
      " --- 循环法计算外积：10.05300000000009ms\n",
      "\n",
      " --- 向量法计算外积：0.45800000000006946ms\n",
      "\n",
      " --- 循环法计算乘积：0.15699999999974068ms\n",
      "\n",
      " --- 向量法计算乘积：0.053999999999998494ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "x1 = np.random.randint(0, 10, 100)\n",
    "x2 = np.random.randint(0, 10, 100)\n",
    "\n",
    "# 经典方法计算点积\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot += x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "\n",
    "print(\"dot=\" + str(dot) + \"\\n --- 循环算法计算用时：\" + str(1000 * (toc - tic)) + \"ms\" )\n",
    "\n",
    "# 向量法计算点积\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1, x2)\n",
    "toc = time.process_time()\n",
    "\n",
    "print(\"dot=\" + str(dot) + \"\\n --- 向量化算法计算用时：\" + str(1000 * (toc - tic)) + \"ms\" )\n",
    "\n",
    "\n",
    "# 经典方法计算外积\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1), len(x2)))\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i][j] = x1[i] * x2[j]\n",
    "toc = time.process_time()\n",
    "\n",
    "print( \"\\n --- 循环法计算外积：\" + str(1000 * (toc - tic)) + \"ms\" )\n",
    "\n",
    "# 向量法计算外积\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1, x2)\n",
    "toc = time.process_time()\n",
    "\n",
    "print( \"\\n --- 向量法计算外积：\" + str(1000 * (toc - tic)) + \"ms\" )\n",
    "\n",
    "# 经典方法计算元素相乘\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "\n",
    "print( \"\\n --- 循环法计算乘积：\" + str(1000 * (toc - tic)) + \"ms\" )\n",
    "\n",
    "# 向量法计算元素乘积\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1, x2)\n",
    "toc = time.process_time()\n",
    "\n",
    "print( \"\\n --- 向量法计算乘积：\" + str(1000 * (toc - tic)) + \"ms\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - 实现 L1 和 L2 loss function\n",
    "\n",
    "在机器学习中，损失值（loss）用来评估模型的表现情况。损失值（loss）越大，说明你的预测值$\\hat{y}$与目标值$y$的差别越大。\n",
    "\n",
    "L1 loss function 定义如下：$L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{i} - \\hat{y}^{i}|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    计算损失值 L1\n",
    "    \n",
    "    参数：\n",
    "    yhat - 预测值 yhat\n",
    "    y - 目标值 y\n",
    "    \n",
    "    返回：\n",
    "    loss 损失值\n",
    "    \"\"\"\n",
    "    loss = np.sum(np.abs(yhat - y))\n",
    "    return loss\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 loss function 定义为：$L_1(\\hat{y}, y) = \\sum_{i=0}^m{(y^{i} - \\hat{y}^{i})}^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    计算损失值 L2\n",
    "    \n",
    "    参数：\n",
    "    yhat - 预测值 yhat\n",
    "    y - 目标值 y\n",
    "    \n",
    "    返回：\n",
    "    loss 损失值\n",
    "    \"\"\"\n",
    "    m = yhat - y\n",
    "    loss = np.sum(np.dot(m, m))\n",
    "    return loss\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
